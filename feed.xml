<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://cnellington.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://cnellington.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-23T17:04:43+00:00</updated><id>https://cnellington.github.io/feed.xml</id><title type="html">blank</title><subtitle>Howdy! Check out my work on personalized models and context-aware meta-learning. </subtitle><entry><title type="html">Hesitation Devastation and a Mathematical Framework for Making High-value Decisions Without Hating Yourself</title><link href="https://cnellington.github.io/blog/2023/househunting/" rel="alternate" type="text/html" title="Hesitation Devastation and a Mathematical Framework for Making High-value Decisions Without Hating Yourself"/><published>2023-04-16T10:00:00+00:00</published><updated>2023-04-16T10:00:00+00:00</updated><id>https://cnellington.github.io/blog/2023/househunting</id><content type="html" xml:base="https://cnellington.github.io/blog/2023/househunting/"><![CDATA[<h3 id="analysis-paralysis-in-the-housing-market">Analysis Paralysis in the Housing Market</h3> <p>For the past week, I’ve been consumed by the highly involved and coordinated dance of apartment hunting. When you enter this dance, you’re doe-eyed and excited to find your cinderella or prince charming. You switch partners a few times, testing the waters and considering your options. Do I prefer the one with central AC or the backyard? Certainly not the one by the train tracks. You’ll leave some suitors disappointed, stepping on a few toes, but feeling like options are in excess. Suddenly your own toes are smashed. “We’ve already received several applications for this unit, we’ll be in touch if they aren’t approved.” They always are. You look around the dance floor, and realize the cinderellas and prince charmings you danced with are leaving. Only Train Tracks remains, and she’s been waiting on you all night.</p> <p>This situation seems common in markets with high demand that also require enormous personal investment. Shopping in this market is a balancing act: you want to find a great, affordable option, but you need to lock in an option fast, before it gets scooped up by another buyer. The size of the investment means most buyers will only rarely visit the market, requiring them to create a new scale every time they’re looking to buy to gauge the quality of their options. But excessive demand means this scale is usually based on just a handful of options they’ve been able to consider in their first day on the market.</p> <p>And so, you’ve danced the dance for one day. One partner was very good, not perfect, but you could see yourself leaving with them. Are they the best option, and your standards are too high? Or are they fairly mediocre, and you have yet to see the best options? You succumb to analysis paralysis, faced with too many potential options to consider and an investment too big to get wrong. You decide to sleep on it, waiting another day to see a few more options. The next morning, the market’s invisible hand provides a swift back-hand across the face when the apartment is unlisted. Your analysis paralysis has subsided, and you are now left with a throbbing case of hesitation devastation.</p> <p>As you can imagine, figuring out when to invest a huge sum of money in a fast-paced market has implications outside of just figuring out which student slumlord to rent from. A more classic version of this problem is directed toward figuring out when to recruit an employee, but it applies the same to apartment hunting, buying a car, getting pretty much anything off of craigslist or facebook marketplace, and even to getting groceries on a time crunch. I’m revisiting this problem for two reasons:</p> <ol> <li>It provides me a timely answer about what apartment to lease.</li> <li>My girlfriend must also acknowledge that my job is cool and useful.</li> </ol> <h3 id="a-probabilistic-approach-to-signing-your-lease">A Probabilistic Approach to Signing Your Lease</h3> <p>To make the right choice with a very limited number of options, you need to figure out how the options you’ve seen compare to the rest of the market you haven’t seen. The spirit is being able to commit after determining when you’ve seen an option that’s “good enough” for you and is likely to compare favorably against the rest of the unseen options. Suppose every option has an overall quality, \(X\), where the quality of an option is something you determine based on attributes you care about. For an apartment, the quality might be based on a tradeoff between price, sqft, amenities, and location.</p> <p><em>Skip ahead to the table if you want to see the framework without the math</em></p> <p>The quality of options in your market is a random variable, and every time you see an option you’re drawing an independent sample from the distribution of the market. For follow-up, a normal distribution probably makes sense where most options are average quality, and much better or much worse options are rarer, but for this example we don’t need to make any assumption about how the market is distributed.</p> \[X \sim P(X)\] <p>To make a choice, we’d like to know with confidence at least \(1 - \delta\) when we’ve seen an option with higher quality than \(\epsilon\) percentage of the market.</p> \[P(max(X_1, ..., X_n) &gt; Z_{\epsilon}) &gt; 1 - \delta\] <p>Where \(Z_{\epsilon}\) is the quality value that is higher than \(\epsilon\) percent of all \(X\) (i.e. the z-score or inverse CDF for our distribution), and we’ll choose how many candidate options \(n\) we should consider before meeting the quality and confidence criteria. To get the same answer, as can figure out how likely it is that none of our options meet a quality threshold.</p> \[P(\bigcup_{i=1}^n [X_i \leq Z_{\epsilon}]) \leq \delta\] \[\Pi_{i=1}^n P(X_i \leq Z_{\epsilon}) \leq \delta\] \[\Pi_{i=1}^n \epsilon \leq \delta\] \[\epsilon^n \leq \delta\] \[n &gt; log_{\epsilon}(\delta)\] <p>Now we have a nice closed-form answer for how many options we should consider before committing! If we need to see an apartment of better quality than \(\epsilon\) percent of our housing market with probability \(1 - \delta\), we can use \(n = log_{\epsilon}(\delta)\) to find the minimum apartments we should tour. This table has the values for \(n\) rounded up to the nearest whole number.</p> <table> <thead> <tr> <th> </th> <th style="text-align: right"> </th> <th style="text-align: right"> </th> <th style="text-align: center"> </th> <th style="text-align: center"> </th> <th style="text-align: center">\(1 - \delta\)</th> <th style="text-align: center"> </th> <th style="text-align: center"> </th> </tr> </thead> <tbody> <tr> <td> </td> <td style="text-align: right"> </td> <td style="text-align: right"> </td> <td style="text-align: center">    \(0.6\)    </td> <td style="text-align: center">    \(0.7\)    </td> <td style="text-align: center">    \(0.8\)    </td> <td style="text-align: center">    \(0.9\)    </td> <td style="text-align: center">    \(0.95\)    </td> </tr> <tr> <td> </td> <td style="text-align: right">\(0.6\)</td> <td style="text-align: right"> </td> <td style="text-align: center">2</td> <td style="text-align: center">3</td> <td style="text-align: center">4</td> <td style="text-align: center">5</td> <td style="text-align: center">6</td> </tr> <tr> <td> </td> <td style="text-align: right">\(0.7\)</td> <td style="text-align: right"> </td> <td style="text-align: center">3</td> <td style="text-align: center">4</td> <td style="text-align: center">5</td> <td style="text-align: center">7</td> <td style="text-align: center">9</td> </tr> <tr> <td>\(\epsilon\)</td> <td style="text-align: right">\(0.8\)</td> <td style="text-align: right"> </td> <td style="text-align: center">5</td> <td style="text-align: center">6</td> <td style="text-align: center">8</td> <td style="text-align: center">11</td> <td style="text-align: center">14</td> </tr> <tr> <td> </td> <td style="text-align: right">\(0.9\)</td> <td style="text-align: right"> </td> <td style="text-align: center">9</td> <td style="text-align: center">12</td> <td style="text-align: center">16</td> <td style="text-align: center">22</td> <td style="text-align: center">29</td> </tr> <tr> <td> </td> <td style="text-align: right">\(0.95\)</td> <td style="text-align: right"> </td> <td style="text-align: center">18</td> <td style="text-align: center">24</td> <td style="text-align: center">32</td> <td style="text-align: center">45</td> <td style="text-align: center">59</td> </tr> </tbody> </table> <p><br/> So if you want to be 90% sure you’ve toured an apartment better than 70% of the market, the table says for \(1 - \delta = 0.9\) and \(\epsilon = 0.7\) you should tour 7 apartments before committing. If you want to be 95% sure you’ve toured an apartment better than 90% of the market, you should tour 45 apartments (not recommended). Anecdotally, I toured 11 apartments before putting a deposit down and I feel confident that I’ve seen at least 1-2 in the top 20% of my market.</p> <p>I’d like to think this is generally helpful for making decisions everywhere when prior information isn’t readily available, such as selecting an employer or employee (and maybe even a romantic partner). It’s a formal way of understanding how many options you should consider, based on your personal threshold for quality, and how confident you need to be in your choice. Although, I probably won’t be using this to see how many pieces of produce I should squeeze at the market before being confident about my choice, unless I need to limit myself to 1-2 MPFS (minutes per fruit selection).</p>]]></content><author><name></name></author><category term="personal"/><category term="life"/><category term="math"/><category term="statistics"/><summary type="html"><![CDATA[House hunting, recruiting, and picking the right fruit at the grocery store]]></summary></entry><entry><title type="html">The How</title><link href="https://cnellington.github.io/blog/2022/thehow/" rel="alternate" type="text/html" title="The How"/><published>2022-11-22T11:00:00+00:00</published><updated>2022-11-22T11:00:00+00:00</updated><id>https://cnellington.github.io/blog/2022/thehow</id><content type="html" xml:base="https://cnellington.github.io/blog/2022/thehow/"><![CDATA[<p>About a year into my PhD, I began noticing a gap in science communication. There was a growing interest in general-audience scientific media, distilling recent cutting-edge research into digestible content on social platforms. I’m an enormous fan of this outlet for non-technical audiences, but it’s often overlooked that technical audiences face the same problem. When a new discovery is published, most of the effort in an article goes toward rationalizing an intellectual contribution, while reproducibility is secondary, and ease of implementation is an afterthought. Because of this, most discoveries take a long time to digest and implement by practitioners but getting new technology in the hands of practitioners is where real impacts are made.</p> <p><em><a href="https://contextualized.ml/">Contextualized.ML</a></em> began as an experiment to fill this gap and get my research to work out-of-the-box for practitioners and instill knowledge as fast as possible. Like most open-source software, it would be modular, tested, and documented in several levels of complexity to cater to a wide range of technical audiences. To clarify intellectual contributions, it would ship with tutorials on the more complex topics.</p> <p>What began as an isolated open-and-shut experiment quickly outgrew my individual ownership and is now a thriving community-driven project. Researchers from around the world contribute to Contextualized.ML because they find the work enjoyable and interesting, and they have been able to implement and improve it without needing months to replicate a pilot study. The development of this software has reflected my development as a scientist, and in both regards I’m far from finished – but always learning and always improving.</p>]]></content><author><name></name></author><category term="personal"/><category term="meta-science"/><category term="entrepreneurship"/><category term="software"/><summary type="html"><![CDATA[An ongoing experiment in community-driven research.]]></summary></entry><entry><title type="html">The Why</title><link href="https://cnellington.github.io/blog/2022/thewhy/" rel="alternate" type="text/html" title="The Why"/><published>2022-11-22T10:00:00+00:00</published><updated>2022-11-22T10:00:00+00:00</updated><id>https://cnellington.github.io/blog/2022/thewhy</id><content type="html" xml:base="https://cnellington.github.io/blog/2022/thewhy/"><![CDATA[<p>Categorizing and classifying diseases is the bedrock of medical research. Defining a disease in terms of behavior and outcomes leads to researching risk factors, discovering mechanisms, creating therapeutics, and developing treatment plans. This research model has produced a wealth of improvements in modern healthcare, but there are critical exceptions; in particular, heterogeneous diseases – diseases like cancer, Alzheimer’s, diabetes, and sepsis – cannot be classified according to shared mechanisms because each patients’ disease manifests and progresses according to a unique, patient-specific mechanism.</p> <p>In these cases, we need a model of disease and a treatment plan that are personalized to each patient but going through the entire process of research and discovery for everyone is infeasible. We require new methods for personalized disease modeling, new meta-science disciplines to do research outside of the traditional one-disease-one-treatment setting, and new businesses that bring these discoveries and tools to afflicted individuals.</p> <p>These needs, combined with the emergence of machine learning paradigms like meta-learning and multi-task learning that formalize the personalization objective, speak to a rich and relatively untapped area of innovation. I see my career as a machine learning researcher, a computational biologist, and a mentor as an opportunity to explore a truly individualized approach to healthcare and train others to grow this budding discipline.</p>]]></content><author><name></name></author><category term="personal"/><category term="meta-science"/><category term="entrepreneurship"/><summary type="html"><![CDATA[Career perspectives at 25]]></summary></entry><entry><title type="html">Evolution of Personalized Models</title><link href="https://cnellington.github.io/blog/2022/personalized_models/" rel="alternate" type="text/html" title="Evolution of Personalized Models"/><published>2022-07-19T10:00:00+00:00</published><updated>2022-07-19T10:00:00+00:00</updated><id>https://cnellington.github.io/blog/2022/personalized_models</id><content type="html" xml:base="https://cnellington.github.io/blog/2022/personalized_models/"><![CDATA[<p><em>This list is not complete, but my end goal is to make it comprehensive. If there is a model you don’t see here that belongs in the set of “principal equations” below, please drop me a line. My email can be copied using the icon on the main page.</em></p> <p>When observed data comes from highly individual, sample-specific models,</p> \[X_i \sim P(X; \theta_i)\] <p>how can we recover \(N\) sample-specific parameter estimates \(\{\widehat{\theta_i}\}_1^N\) with just \(N\) samples?</p> <p>Recently, I’ve been working on a survey of personalized modeling methods that answer this question with various assumptions about the relationships within a set models. This interesting lineage of statistics has had sporadic contributions over its lifetime, and is rarely viewed as a single family of methods. I’m writing this survey to unify this lineage and to raise awareness – especially among clinicians and biologists – that there is a suite of statistically-grounded methods for inferring personalized models that are well-aligned with our understanding of many real biological systems, while requiring little more effort to apply than classic subpopulation/cohort models.</p> <h3 id="0-subpopulation-and-cohort-models">0. Subpopulation and Cohort Models</h3> <p>First, an explanation of subpopulation and cohort models and their issues. We will assume that the parameters \(\theta\) for our model of interest have some log-likelihood function \(\ell(X; \theta)\) over the data \(X\). For some subpopulation or cohort \(c\), we can model this cohort with a generic estimator, maximizing the likelihood of the cohort model on the cohort data.</p> \[\widehat{\theta_c} = \arg\max_{\theta_c} \ell(X_c; \theta_c)\] <p>This assumes that each subpopulation or cohort is homogeneous, having the same underlying model. However, in many cases biological mechanisms are extremely heterogeneous and it would better align with our understanding if model estimates reflected the differences between samples.</p> <h1 id="personalized-models">Personalized models</h1> <p>From here onward, I <strong>very briefly</strong> outline an assumption and a principal equation that applies this assumption to recover \(N\) models from \(N\) samples. I keep these equations simple to highlight the assumption-model relationship in each case.</p> <h2 id="context-informed-models">Context-informed models</h2> <p>Context variables like time or location (which we call <em>covariates</em>) can often explain model variation.</p> <h3 id="1-classic-varying-coefficient-models">1. Classic varying-coefficient models</h3> <p>Assumption: models with similar covariates have similar parameters (more formally: parameters are smooth over covariates)</p> \[\widehat{\theta}_0, ..., \widehat{\theta}_N = \arg\max_{\theta_0, ..., \theta_N} \sum_{i, j} \frac{K(c_i, c_j)}{\sum_{k} K(c_i, c_k)} \ell(x_j; \theta_i)\] <p>Where \(K(c_i, c_j)\) is a similarity metric, usually a kernel.</p> <p>Example: <a href="https://doi.org/10.1214/09-AOAS308">Kolar, Mladen et al. “Estimating Time-Varying Networks.”</a></p> <h3 id="2-linear-varying-coefficient-models">2. Linear varying-coefficient models</h3> <p>Assumption: parameters vary linearly with covariates. Stronger than the classic assumption but highly interpretable. This allows the relationship between covariates and parameters to itself be parameterized.</p> \[\widehat{A} = \arg\max_A \sum_i \ell(x_i; A c_i)\] \[\widehat{\theta}_0, ..., \widehat{\theta}_N = \widehat{A} C^T\] <p>Example: <a href="https://doi.org/10.1214/aos/1017939139">Fan, Jianqing, and Wenyang Zhang. “Statistical Estimation in Varying Coefficient Models.”</a></p> <h3 id="3-contextualized-models">3. Contextualized Models</h3> <p>Assumption: parameters are some function of context, but make no assumption on the form of that function.</p> \[\widehat{f} = \arg \max_{f \in \mathcal{F}} \sum_i \ell(x_i; f(c_i))\] \[\widehat{\theta}_0, ..., \widehat{\theta}_N = \widehat{f}(C)\] <p><a href="https://doi.org/10.48550/arXiv.1705.10301">Al-Shedivat, Maruan, et al. “Contextual Explanation Networks.”</a></p> <h2 id="latent-structure-models">Latent-structure Models</h2> <p>These methods discover underlying structures in the data that guide model learning.</p> <h3 id="4-partition-models">4. Partition models</h3> <p>Assumption: parameters can be partitioned into homogeneous groups over the covariate space, but make no assumption about where these partitions occur. Partition model estimators are most often utilized to infer abrupt model changes over time.</p> \[\widehat{\theta}_0, ..., \widehat{\theta}_N = \arg\max_{\theta_0, ..., \theta_N} \sum_i \ell(x_i; \theta_i) + \sum_{i = 2}^N \text{TV}(\theta_i, \theta_{i-1})\] <p>Where the regularizaiton term might take the form \(\text{TV}(\theta_i, \theta_{i - 1}) = |\theta_i - \theta_{i-1}|\)</p> <p>Example: <a href="https://doi.org/10.1214/09-AOAS308">Kolar, Mladen et al. “Estimating Time-Varying Networks.”</a></p> <h2 id="context-informed-latent-structure-models">Context-informed latent-structure models</h2> <p>These methods use principals from both context-informed and latent-structure methods.</p> <h3 id="5-distance-matching-models">5. Distance-matching models</h3> <p>Assumption: there is a transformation of the context space where distance in the transformed space is equal to the similarity between models. This uses the similar-context-similar-parameter assumption from varying-coefficient models, but also imposes a different-context-different-parameter assumption that doesn’t directly follow the first.</p> \[\widehat{\theta}_0, ..., \widehat{\theta}_N = \arg\max_{\theta_0, ..., \theta_N, D} \sum_{i=0}^N \prod_{j=0 s.t. D(c_i, c_j) &lt; d}^N \ell(x_j; \theta_i) P(\theta_i ; \theta_j)\] <p>Where \(D(c_i, c_j)\) is a learnable distance metric.</p> <p>Example: <a href="https://doi.org/10.48550/arXiv.1910.06939">Lengerich, Benjamin, et al. “Learning Sample-Specific Models with Low-Rank Personalized Regression.”</a></p> <p>Thanks for reading!</p> <p><em>This list is not complete, but my end goal is to make it comprehensive. If there is a model you don’t see here that belongs in the set of “principal equations” below, please drop me a line. My email can be copied using the icon on the main page.</em></p>]]></content><author><name></name></author><category term="research"/><category term="math"/><category term="machine-learning"/><category term="statistics"/><category term="personalization"/><summary type="html"><![CDATA[Unifying a statistical lineage]]></summary></entry><entry><title type="html">This post is a test</title><link href="https://cnellington.github.io/blog/2022/firstpost/" rel="alternate" type="text/html" title="This post is a test"/><published>2022-07-14T23:40:16+00:00</published><updated>2022-07-14T23:40:16+00:00</updated><id>https://cnellington.github.io/blog/2022/firstpost</id><content type="html" xml:base="https://cnellington.github.io/blog/2022/firstpost/"><![CDATA[<p>Hello World</p>]]></content><author><name></name></author><category term="test"/><category term="category"/><category term="test"/><category term="tag"/><summary type="html"><![CDATA[test description]]></summary></entry></feed>